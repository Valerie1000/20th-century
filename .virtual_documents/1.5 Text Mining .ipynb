





from textblob import TextBlob 
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib
import nltk
import re
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
from collections import Counter
sns.set()


# need another import
import nltk
nltk.download('punkt_tab')


import nltk
nltk.download('averaged_perceptron_tagger_eng')





# Import txt file

with open('key_events_20th_century.txt', 'r', errors='ignore') as file: 
   data = file.read().replace( '\n', ' ')





# verify that data is indeed a string
type(data)


# Word tokenization

from nltk.tokenize import word_tokenize
tokenized_word = word_tokenize(data)
print(tokenized_word)


# Create frequency distribution

from nltk.probability import FreqDist
dist_words = FreqDist(tokenized_word)
print(dist_words)


dist_words.most_common(10)


# Frequency Distribution Plot

plt.figure(figsize=(8, 3))
dist_words.plot(10,cumulative = False)
plt.show()





# Defining stopwords

from nltk.corpus import stopwords
stop_words = set(stopwords.words("english"))
print(stop_words)


# Removing stopwords in words

filtered_words = [] # creates an empty list
for word in tokenized_word:
    if word not in stop_words:
        filtered_words.append(word)


filtered_words


# Create a new FreqDist for filtered_words

dist_words_filter = FreqDist(filtered_words)
print(dist_words_filter)


# Frequency Distribution Plot

plt.figure(figsize=(8, 3))
dist_words_filter.plot(10, cumulative = False)
plt.show()


dist_words_filter


# Substitute all punctuations marks with a space 

sans_punc = re.sub("[^a-zA-Z]",  # Search for all non-letters
                          " ",          # Replace all non-letters with spaces
                          str(filtered_words))


sans_punc


# Word tokenization

tokenized_word_2 = word_tokenize(sans_punc)
print(tokenized_word_2)


# Create a new FreqDist

dist_words_filter_2 = FreqDist(tokenized_word_2)


# Frequency Distribution Plot

plt.figure(figsize=(8, 3))
dist_words_filter_2.plot(30, cumulative = False)
plt.show()





dist_words_filter_2.most_common(20)


new_stopwords = ["The", "s", 'ISBN', 'In', 'th', 'e', 'on', 'com']


filtered = []
for word in dist_words_filter_2:
    if word not in new_stopwords:
        filtered.append(word)





%%time
text = TextBlob(str(filtered))


text


tags_list = text.tags


tags_list





df_text = pd.DataFrame(tags_list)
df_text.columns = ['Words', "Word type"]


df_text.head()


df_t = df_text.groupby('Word type').count().reset_index()


df_t.head()


top10 = df_t.nlargest(20, 'Words')





plt.figure(figsize = (10, 5))
with sns.dark_palette("xkcd:blue", 20):
    sns.barplot(x = "Words", y = "Word type",
    saturation = 0.9, data = top10).set_title("Key Events 20th Century - top 10 word types used")





#Check for case sensitivity
df_text['Words'] = df_text['Words'].str.lower()


# Remove punctuation and special characters
df_text['Words'] = df_text['Words'].str.replace(r'[^\w\s]', '', regex=True)





df_noun = df_text[(df_text['Word type'] == "NN") | (df_text['Word type'] == "NNS") | (df_text['Word type'] == "NNP")]
df_noun.columns = ["Word", "Occurences"]
x = df_noun.groupby('Word').count().reset_index()
y = x.sort_values(by =['Occurences'], ascending=False)
top15_noun = y.nlargest(15, 'Occurences')


top15_noun


plt.figure(figsize=(10, 5))
with sns.dark_palette("xkcd:blue", 10):
    sns.barplot(x="Word", y="Occurences",
    saturation=0.9, data = top15_noun).set_title("Key events 20th centory - most frequently used nouns")








df_verb = df_text[(df_text['Word type'] == "VB")  | (df_text['Word type'] == "VBD")]
df_verb.columns = ["Word", "Occurences"]
x = df_verb.groupby('Word').count().reset_index()
y = x.sort_values(by = ['Occurences'], ascending=False)
top15_verb = y.nlargest(15, 'Occurences')


top15_verb


plt.figure(figsize = (10, 5))
with sns.dark_palette("xkcd:blue", 10):
    sns.barplot(x = "Word", y = "Occurences",
    saturation = 0.9, data = top15_verb).set_title("Key event 20th century - most frequently used verbs")
    plt.xticks(rotation=45) 





df_adjective = df_text[df_text['Word type'] == "JJ"]
df_adjective.columns = ["Word", "Occurences"]
x = df_adjective.groupby('Word').count().reset_index()
y = x.sort_values(by=['Occurences'], ascending=False)
top15_adjective = y.nlargest(15, 'Occurences')


top15_adjective


plt.figure(figsize=(10, 5))
with sns.dark_palette("xkcd:blue", 10):
    sns.barplot(x="Word", y="Occurences",
    saturation=0.9, data=top15_adjective).set_title("Key event 20th century - most frequently used adjectives")
    plt.xticks(rotation=45) 








# Load country names
df_countries = pd.read_csv("countries_list_20th_century_1.5.csv")
country_list = df_countries['country_name'].str.strip().str.lower().tolist()


# Lowercase and clean the text
text_clean = text.lower()



text_str = str(text_clean)

country_mentions = {
    country: text_str.count(country)
    for country in country_list
    if text_str.count(country) > 0
}



df_mentions = pd.DataFrame(list(country_mentions.items()), columns=['Country', 'Mentions'])
df_mentions = df_mentions.sort_values(by='Mentions', ascending=False).reset_index(drop=True)


df_mentions





plt.figure(figsize=(5, 8))
sns.barplot(x='Mentions', y='Country', data=df_mentions, hue = 'Country', palette='Blues_d')

plt.title("Country Mentions in 20th Century Key Events Text")
plt.xlabel("Number of Mentions")
plt.ylabel("Country")
plt.tight_layout()
plt.show()








# Combine list of text into one string
text_blob = TextBlob(" ".join(filtered))

# Get sentiment
print("Polarity:", text_blob.sentiment.polarity)        # Range: [-1, 1]
print("Subjectivity:", text_blob.sentiment.subjectivity) # Range: [0, 1]


# Sentiment for each event
sentiments = [
    {
        "event": text,
        "polarity": TextBlob(text).sentiment.polarity,
        "subjectivity": TextBlob(text).sentiment.subjectivity
    }
    for text in filtered
]

df_sentiment = pd.DataFrame(sentiments)

# Show or plot top positive/negative events
df_sentiment.sort_values(by='polarity', ascending=True).head(5)  # Most negative
df_sentiment.sort_values(by='polarity', ascending=False).head(5) # Most positive




